---
title: "Data Project Team GoBills"
author: "STAT 420, Summer 2023, Naveen Baskaran (nc42), Frank Salamone (frankns2), Aleksandr Stpenko(as99)"
date: ''
output:
  html_document:
    theme: readable
    toc: yes
  word_document:
    toc: yes
  pdf_document: default
urlcolor: cyan
---


***

### 1. The names of the students who will be contributing to the group project.

Naveen Baskaran (nc42)
Frank Salamone (frankns2)
Aleksandr Stepenko (as99)

### 2. A tentative title for the project.

“Predicting Airbnb Prices in London on Weekends”

### 3. Description of the data file (what they contain including number of variables and number of records). You do not necessarily have to list all the variables, but at least mention those of greatest importance.

Our dataset concerns Airbnb rentals in London on the weekends.  It contains 19 variables in this dataset with 5379 records. These records consist of information regarding the characteristics of each rental, and the total price of the listing.  Our dataset is a subset of a larger collection of Airbnb listings in many European cities.

The most important variable is realSum, which is the total price of the listing, and will serve as our response.  There are many categorical variables, including, most importantly,  room_type, room_shared, and host_is_superhost.  The distance to city center, dist, and metro_dist, the distance to the nearest metro station, will be important continuous numeric variables.  

### 4. Background information on the data sets, including specific citation of their source (so that I can also access it).

This dataset was found on Kaggle. Briefly, it is a part of a larger set of Airbnb prices in European cities.  Please see the dataset description below (taken from Kaggle).

https://www.kaggle.com/datasets/thedevastator/airbnb-prices-in-european-cities?resource=download

“This dataset provides a comprehensive look at Airbnb prices in some of the most popular European cities. Each listing is evaluated for various attributes such as room types, cleanliness and satisfaction ratings, bedrooms, distance from the city center, and more to capture an in-depth understanding of Airbnb prices on both weekdays and weekends. Using spatial econometric methods, we analyze and identify the determinants of Airbnb prices across these cities. Our dataset includes information such as realSum (the total price of the listing), room_type (private/shared/entire home/apt), host_is_superhost (boolean value indicating if host is a superhost or not), multi (indicator whether listing is for multiple rooms or not), biz (business indicator) , guest_satisfaction_overall (overall rating from guests comparing all listings offered by host ), bedrooms, dist (distance from city center) , lng & lat coordinates for location identification etc. We hope that this data set offers insight into how global markets are affected by social dynamics and geographical factors which in turn determine pricing strategies for optimal profitability!”

### 5. A brief statement of the business, science, research, or personal interest you have in the data set which you hope to explore. 

From an outsider’s perspective, the Airbnb platform has revolutionized the renting of rooms much as Uber has revolutionized ridesharing.  Airbnb allows users of the platform to monetize property that they would not be able to otherwise.  In order to determine if renting out a room is a good business case or worth the owner’s time, it would be helpful to have an estimate of how much income the room would generate.  Our proposal is to develop a model, trained on the London Weekend Airbnb data, that could predict the room rental income based on predictors known to the person putting the room up for rent.  

### 6. Evidence that the data can be loaded into R. Load the data, and print the first few values of the response variable as evidence.

**Solution**

```{r}
london = read.csv("london_weekends.csv")
head(london$realSum)
head(london)
nrow(london)
ncol(london)
```


Methods
First part:
Exploratory analysis.  Lay out what variables look like they are related to the response through correlation
analysis.  Scatterplot matrix.  Basic analysis.  Hint on possibility of collinearity.
Second part:
Some connector between predictors and response in linear fashon.  Main effects analysis.
Aren't completely happy.  Give RMSE, RSE, adjusted r^2.
Next try to remove predictors to increase explanatory power.
Third part:
Try transformations, interactions.  Polynomial terms.
Do backward elimination from your big model from above.
Go in same order as introduced in the course.
Model assumptions
Train and test split

Results
Dont' characterize all models.  2 or 3 or 4models that seem to be the best.  Say why fully additive model
didn't work.  Show progress that you made.  End results.  Hit the highlights.
Discussion
Table with candidate model, with RMSC, residuals, assumptions.  Argue why model is the best.

## Introduction

rubric:  Introduction  Lay everything out in terms of data.  Objective of what want to learn about data.

From an outsider’s perspective, the Airbnb platform has revolutionized the renting of rooms much as Uber has revolutionized ridesharing.  Airbnb allows users of the platform to monetize property that they would not be able to otherwise.  In order to determine if renting out a room is a good business case or worth the owner’s time, it would be helpful to have an estimate of how much income the room would generate.  Our proposal is to develop a model, trained on the London Weekend Airbnb data, that could predict the room rental income based on predictors known to the person putting the room up for rent.  The data is available on Kaggle at https://www.kaggle.com/datasets/thedevastator/airbnb-prices-in-european-cities, and from the original authors, Gyódi, Kristóf and Nawaro, Łukasz at https://zenodo.org/record/4446043#.Y9Y9ENJBwUE.

The columns of the database include:

realSum: the full price of accommodation for two people and two nights in EUR
room_type: the type of the accommodation 
room_shared: dummy variable for shared rooms
room_private: dummy variable for private rooms
person_capacity: the maximum number of guests 
host_is_superhost: dummy variable for superhost status
multi: dummy variable if the listing belongs to hosts with 2-4 offers
biz: dummy variable if the listing belongs to hosts with more than 4 offers
cleanliness_rating: cleanliness rating
guest_satisfaction_overall: overall rating of the listing
bedrooms: number of bedrooms (0 for studios)
dist: distance from city centre in km
metro_dist: distance from nearest metro station in km
attr_index: attraction index of the listing location
attr_index_norm: normalised attraction index (0-100)
rest_index: restaurant index of the listing location
attr_index_norm: normalised restaurant index (0-100)
lng: longitude of the listing location
lat: latitude of the listing location

What are the dimensions of the dataset?

```{r}
nrow(london)
ncol(london)
```

There are 5379 rows of data with 20 variables, as described above.

## Methods

Rubric:
Methods
First part:
Exploratory analysis.  Lay out what variables look like they are related to the response through correlation
analysis.  Scatterplot matrix.  Basic analysis.  Hint on possibility of collinearity.
Second part:
Some connector between predictors and response in linear fashon.  Main effects analysis.
Aren't completely happy.  Give RMSE, RSE, adjusted r^2.
Next try to remove predictors to increase explanatory power.
Third part:
Try transformations, interactions.  Polynomial terms.
Do backward elimination from your big model from above.
Go in same order as introduced in the course.


### Data Cleaning

There are several variables that need to be converted to factors.  These include: room_type, room_shared, room_private, and host_is_superhost.  

```{r}
unique(london$room_type) 
unique(london$room_shared)
unique(london$room_private)
unique(london$host_is_superhost)
```

The room_type column can converted into a 3 level factor variable with the levels being: "Private room", "Entire home/apt", and "Shared room".

Another question is if room_shared and room_private are independent.

```{r}
mean(london$room_shared == london$room_private)
```

Based on the data, they mean different things.

Columns multi and biz are factors stored as integers.

```{r}
unique(london$multi)
unique(london$biz)
```

Switch these to True and False.

```{r}
london$multi[london$multi == 0] = "False"
london$multi[london$multi == 1] = "True"
london$biz[london$biz == 0] = "False"
london$biz[london$biz == 1] = "True"
```

The bedrooms and person_capacity column should also be changed to a factor.

```{r}
unique(london$bedrooms)
unique(london$person_capacity)
```

Switch all to factors, also keep a copy of london with bedrooms and person_capacity as integers 

```{r}
london$room_type = as.factor(london$room_type)
london$room_shared = as.factor(london$room_shared)
london$room_private = as.factor(london$room_private)
london$multi = as.factor(london$multi)
london$biz = as.factor(london$biz)
london$host_is_superhost = as.factor(london$host_is_superhost)
london_less_factors = london
london$person_capacity = as.factor(london$person_capacity)
london$bedrooms = as.factor(london$bedrooms)
head(london)
head(london_less_factors)
```

Variables attr_index and attr_index_norm will be collinear, as will rest_index and rest_index_norm, so the non-normalized columns are removed from the dataframe.  Also, on initial analysis of linear models, it was apparent that room_shared and room_private are exactly collinear with room_type, so they will be removed.  Also, remove the index X

```{r}
cor(london$attr_index, london$attr_index_norm)
cor(london$rest_index, london$rest_index_norm)
london = subset(london, select = -c(attr_index, rest_index, room_shared, room_private, X))
london_less_factors = subset(london_less_factors, select = -c(attr_index, rest_index, room_shared, room_private, X))
```


See if there are any NAs:

```{r}
sum(is.na(london))
```

```{r}
head(london)
head(london_less_factors)
```


### Exploratory Analysis

```{r}
diag = function(model, name){
  resid = fitted(model) - london$realSum
  rmse = sqrt(sum(resid^2)/nrow(london))
  r2 = summary(model)$r.squared
  coefs = nrow(summary(model)$coeff)
  data.frame(model = name, rmse = rmse, r2 = r2, coefficients = coefs )
}
```


#### Main Effects Analysis

```{r}
full_additive_mod = lm(realSum ~ ., data = london)
summary(full_additive_mod)
linear_exp_results = diag(full_additive_mod, "full_additive_mod")
linear_exp_results
```
Fitting an additive model with no interactions to the data yields an $R^2 = 0.2769$.  This seems quite low.  

Bedrooms and person_capacity have a large number of values, maybe a model with them as numbers, not factors would give a better fit.

```{r}
full_additive_less_factors_mod = lm(realSum ~ ., data = london_less_factors)
summary(full_additive_less_factors_mod)
row = diag(full_additive_less_factors_mod, "full_additive_less_factors_mod")
row
linear_exp_results = rbind(linear_exp_results, row)
```
Using bedrooms and person_capacity gives a slightly worse $R^2 = 0.2581$, as compared to $R^2 = 0.2769$ as factors.

We will use AIC to determine if any of the predictors can be dropped.

```{r}
aic_full_additive_mod = step(full_additive_mod, direction = "backward", trace = FALSE)
summary(aic_full_additive_mod)
row = diag(aic_full_additive_mod, "aic_full_additive_mod")
row
linear_exp_results = rbind(linear_exp_results, row)
```
AIC eliminates just host_is_superhostTrue, multiTrue, bizTrue, cleanliness_rating, rest_index_norm, and lat.  The $R^2$ value of this model left after AIC elimination is almost the same at 0.2768 as compared to the full additive model with an $R^2$ of 0.2769. 

We will now try BIC to see if this results in a smaller model.

```{r}
bic_full_additive_mod = step(full_additive_mod, direction = "backward", k = log(nrow(london)), trace = FALSE)
summary(bic_full_additive_mod)
row = diag(bic_full_additive_mod, "bic_full_additive_model")
row
linear_exp_results = rbind(linear_exp_results, row)
```

BIC, as we would expect, produces a smaller model.

As there is little difference between these models, we will continue the analysis with the full linear model.  In summary, no one simple additive model stands out.  When we fit the model with bedrooms and person_capacity as integers as in full_additive_less_factors_mod, we get a poorer fit.

```{r}
library(knitr)
kable(linear_exp_results)
```


```{r}

```


#### Collinearity Analysis

Although collinearity likely will not affect prediction, it will affect our ability to perform inference tests.

```{r}
london_numeric = subset(london, select = c(cleanliness_rating, guest_satisfaction_overall, dist, metro_dist, attr_index_norm, rest_index_norm))
pairs(london_numeric)
```

Some of these data show collinearity and some look non-linear.  In particular, distance and metro_distance seem to be collinear.

Let us check for collinearity using the variance inflation factor.

```{r}
library(faraway)
vif(full_additive_mod)
vif(full_additive_mod)[unname(vif(full_additive_mod)) > 5]
``` 

Per the textbook, predictors with a VIF of more than 5 are suspicious for collinearity.  The variables with VIF values more than 5 include dist, attr_index_norm, and rest_index_norm. This makes sense as dist is the distance to the city center, and it is likely that if this is small, the number of attractions and restaurants (as measured by attr_index_norm and rest_index_norm) would be high.  There are more attractions and restaurants near the city center.  Also, metro_dist, the distance to a metro station, would likely be small if dist is small. These values are not tremendously larger than 5, so we will keep them in model.

#### Transformation Analysis

What does the distribution of the room prices look like?

```{r}
hist(london$realSum)
```

Maybe this would look better if we took the logarithm.  This is a typical transformation for prices that can vary over several orders of magnitude.  

```{r}
hist(log(london$realSum), prob = TRUE)
curve(dnorm(x, mean = mean(log(london$realSum)), sd = sd(log(london$realSum))), 
      col = "darkorange", add = TRUE, lwd = 3)
```

This does look much better.  The values are more spread out, and approximate a normal distribution.  Let us try to fit a model with the log transformation of the response.

```{r}
log_diag = function(model, name){
  resid = exp(fitted(model)) - london$realSum
  rmse = sqrt(sum(resid^2)/nrow(london))
  r2 = summary(model)$r.squared
  coefs = nrow(summary(model)$coeff)
  data.frame(model = name, rmse = rmse, r2 = r2, coefficients = coefs )
}
```

```{r}
full_log_model = lm(log(realSum) ~ ., data = london)
summary(full_log_model)
log_diag(full_log_model, "full_log_model")
```
The log transformation gives a much better $R^2$.  The $R^2$ value increased from around 0.27 to 0.6878 with the log transform.  The log model also has much lower RMSE.

We will now try transformations of other variables.

```{r}
hist(log(london$dist))
```

```{r}
hist(sqrt(london$dist), prob = TRUE)
curve(dnorm(x, mean = mean(sqrt(london$dist)), sd = sd(sqrt(london$dist))), 
      col = "darkorange", add = TRUE, lwd = 3)
```

```{r}
hist(log(london$guest_satisfaction_overall))
```

```{r}
hist((london$guest_satisfaction_overall) ^ 3, prob = TRUE)
curve(dnorm(x, mean = mean((london$guest_satisfaction_overall)^3), sd = sd((london$guest)^3)), 
      col = "darkorange", add = TRUE, lwd = 3)
```


```{r}
hist(london$rest_index_norm)
```


```{r}
hist(log(london$rest_index_norm))
```

```{r}
hist(sqrt(london$dist))
```

Maybe transformations of other variables may be of help.


```{r}
print("Baseline R^2 value for the full additive log model with all predictors and log transformation of the response realSum")
summary(full_log_model)$r.squared

transform_results = log_diag(lm(log(realSum) ~ . + I(dist^2), data = london), "dist^2")
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(dist^3), data = london), "dist^3"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(sqrt(dist)), data = london), "sqrt(dist)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(log(dist)), data = london), "log(dist)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(1/dist), data = london), "1/dist"))

transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(metro_dist^2), data = london), "metro_dist^2"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(metro_dist^3), data = london), "metro_dist^3"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(sqrt(metro_dist)), data = london), "sqrt(metro_dist)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(log(metro_dist)), data = london), "log(metro_dist)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(1/metro_dist), data = london), "1/metro_dist"))

transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(cleanliness_rating^2), data = london), "cleanliness_rating^2"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(cleanliness_rating^3), data = london), "cleanliness_rating^3"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(sqrt(cleanliness_rating)), data = london), "sqrt(cleanliness_rating)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(log(cleanliness_rating)), data = london), "log(cleanliness_rating)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(1/cleanliness_rating), data = london), "1/cleanliness_rating"))

transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(guest_satisfaction_overall^2), data = london), "guest_satisfaction_overall^2"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(guest_satisfaction_overall^3), data = london), "guest_satisfaction_overall^3"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(sqrt(guest_satisfaction_overall)), data = london), "sqrt(guest_satisfaction_overall)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(log(guest_satisfaction_overall)), data = london), "log(guest_satisfaction_overall)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(1/guest_satisfaction_overall), data = london), "1/guest_satisfaction_overall"))

transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(attr_index_norm^2), data = london), "attr_index_norm^2"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(attr_index_norm^3), data = london), "attr_index_norm^3"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(sqrt(attr_index_norm)), data = london), "sqrt(attr_index_norm)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(log(attr_index_norm)), data = london), "log(attr_index_norm)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(1/attr_index_norm), data = london), "1/attr_index_norm"))

transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(rest_index_norm^2), data = london), "rest_index_norm^2"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(rest_index_norm^3), data = london), "rest_index_norm^3"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(sqrt(rest_index_norm)), data = london), "sqrt(rest_index_norm)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(log(rest_index_norm)), data = london), "log(rest_index_norm)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(1/rest_index_norm), data = london), "1/rest_index_norm"))

transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(lat^2), data = london), "lat^2"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(lat^3), data = london), "lat^3"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(sqrt(lat)), data = london), "sqrt(lat)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(log(lat)), data = london), "log(lat)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(1/lat), data = london), "1/lat"))

transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(lng^2), data = london), "lng^2"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(lng^3), data = london), "lng^3"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(sqrt(lng)), data = london), "sqrt(lng)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(log(lng)), data = london), "log(lng)"))
transform_results = rbind(transform_results, log_diag(lm(log(realSum) ~ . + I(1/lng), data = london), "1/lng"))

transform_results

```

The biggest improvement in terms of RMSE seems to be with guest_satisfaction_overall^3.

#### Interaction Analysis

Next do a model with interactions, and then reduce the number of variables with backward AIC.  We will start with the full log model.

```{r}
full_interact = lm(log(realSum) ~ .^2, data = london)
interact_results = log_diag(full_interact, "full_interact")
interact_results
```

The full interaction model has a higher $R^2$ value than any of the transformation models above.  We will have to check if there is overfitting.The full_interact model has a significantly lower RMSE than the transformation models.

The full interaction model is quite large.  Let us use AIC and BIC to reduce the number of predictors.

```{r}
aic_full_interact = step (full_interact, direction = "backward", trace = FALSE)
row = log_diag(aic_full_interact, "aic_full_interact")
row
interact_results = rbind(aic_full_interact, "aic_full_interact")
```

AIC decreases the number of coefficients from 214 in the full interaction model to 113 with a decrease in $R^2$ from 0.7335292 to 0.7294467.

Try BIC to get a smaller model

```{r}
bic_full_interact = step (full_interact, direction = "backward", trace = FALSE, k = log(nrow(london)))
row = log_diag(bic_full_interact, "bic_full_interact")
row
interact_results = rbind(bic_full_interact, "bic_full_interact")
```

BIC decreases the number of coefficients from 214 in the full interaction model to 45 with a decrease in $R^2$ from 0.7335292 to 0.7171065.

#### Putting It Together

Try a model with transformations and interactions.  Start with the model from the BIC elimination above and add the 1/rest_index_norm transformation.

```{r}
combined_model =  lm(log(realSum) ~ room_type + person_capacity + multi + 
    biz + cleanliness_rating + guest_satisfaction_overall + guest_satisfaction_overall^3 + bedrooms + 
    dist + metro_dist + attr_index_norm + rest_index_norm + lng + 
    lat + room_type:bedrooms + multi:guest_satisfaction_overall + 
    biz:cleanliness_rating + biz:lat + cleanliness_rating:guest_satisfaction_overall + 
    cleanliness_rating:attr_index_norm + bedrooms:attr_index_norm + 
    dist:metro_dist + dist:rest_index_norm + dist:lng + dist:lat + 
    metro_dist:attr_index_norm + metro_dist:rest_index_norm + 
    attr_index_norm:rest_index_norm + rest_index_norm:lat, data = london)

combined_results = log_diag(combined_model, "combined_model")
combined_results
```

Adding 1/rest_index_norm marginally increases $R^2$.


Let us remove predictors to increase explanatory power.  I removed predictors with collinearity and low p-values 

```{r}
smaller_combined_model =  lm(log(realSum) ~ room_type + person_capacity + cleanliness_rating + guest_satisfaction_overall
                     + bedrooms + dist + attr_index_norm  + lng + 
                     + multi:guest_satisfaction_overall + biz:cleanliness_rating                     
                     + cleanliness_rating:guest_satisfaction_overall                        
                     + dist:rest_index_norm + metro_dist:attr_index_norm, data = london)

vif(smaller_combined_model)
row = log_diag(smaller_combined_model, "smaller_combined_model")
row
combined_results = rbind(smaller_combined_model, "smaller_combined_model")
```

#### Assumption Analysis

```{r}
assumpt = function(model, pcol = "gray", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = TRUE){
  if (plotit == TRUE){
    par(mfrow=c(1,2)) 
    plot(fitted(model), resid(model), col = pcol, pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = paste("Fitted vs. Residuals for ", substitute(model), sep = ""))
    abline(h = 0, col = lcol, lwd = 2)
    qqnorm(resid(model), main = paste("Normal Q-Q Plot for ", substitute(model), sep = ""), col = pcol, pch = 20)
    qqline(resid(model), col = lcol, lwd = 2)
  }
  if (testit == TRUE){
    #test = shapiro.test(resid(model))$p.value
    #res = ifelse (test > alpha, "Fail to Reject", "Reject")
    #list("p_val" = test, "decision" = res)
  }
}
```

```{r}
assumpt(combined_model)
assumpt(smaller_combined_model)
```


```{r}
hist(fitted(smaller_combined_model))
```


```{r}
hist(log(london$realSum))
```

It looks like the log transformation of the response is not perfect.  We will try the Box-Cox transformation.

```{r}
library(MASS)
bc = boxcox(smaller_combined_model)
(lambda = bc$x[which.max(bc$y)])
```
```{r}
invBoxCox = function(x, lambda)
    		if (lambda == 0) exp(x) else (lambda*x + 1)^(1/lambda)
```



```{r}
bc_combined_model =  lm(((realSum^lambda-1)/lambda) ~ room_type + person_capacity + cleanliness_rating + guest_satisfaction_overall
                     + bedrooms + dist + attr_index_norm  + lng + 
                     + multi:guest_satisfaction_overall + biz:cleanliness_rating                     
                     + cleanliness_rating:guest_satisfaction_overall                        
                     + dist:rest_index_norm + metro_dist:attr_index_norm, data = london)
assumpt(bc_combined_model)
hist(((london$realSum^lambda-1)/lambda), prob = TRUE)
curve(dnorm(x, mean = mean(((london$realSum^lambda-1)/lambda)), sd = sd(((london$realSum^lambda-1)/lambda))), 
      col = "darkorange", add = TRUE, lwd = 3)
residuals = unname(resid(bc_combined_model))[1:4998]
shapiro.test(residuals)
resid = invBoxCox(fitted(bc_combined_model), lambda) - london$realSum
rmse = sqrt(sum(resid^2)/nrow(london))
r2 = summary(bc_combined_model)$r.squared
coefs = nrow(summary(bc_combined_model)$coeff)
data.frame(model = "bc_combined_model", rmse = rmse, r2 = r2, coefficients = coefs )
```

```{r}
bc = boxcox(full_additive_mod)
(lambda = bc$x[which.max(bc$y)])
```
```{r}
bc_combined_model =  lm(((realSum^lambda-1)/lambda) ~ ., data = london)
assumpt(bc_combined_model)
```


```{r}
glm.mod <- glm(realSum ~ ., data = london, family=gaussian(link="log"))
summary(glm.mod)
```

#### Removal of Unusual Values

In the above analysis, it looks as if the smaller_combined_model works the best.  We will now eliminate unusual values.

```{r}
plot(smaller_combined_model)
unusual_index = cooks.distance(smaller_combined_model)>(4/nrow(london))
london_no_unusual = london[!unusual_index,]
```

```{r}
smaller_combined_no_unusual =  lm(log(realSum) ~ room_type + person_capacity + cleanliness_rating + guest_satisfaction_overall
                     + bedrooms + dist + attr_index_norm  + lng + 
                     + multi:guest_satisfaction_overall + biz:cleanliness_rating                     
                     + cleanliness_rating:guest_satisfaction_overall                        
                     + dist:rest_index_norm + metro_dist:attr_index_norm, data = london_no_unusual)
log_diag(smaller_combined_no_unusual, "smaller_combined_no_unusual")
assumpt(smaller_combined_no_unusual)
plot(smaller_combined_no_unusual)
shapiro.test(resid(smaller_combined_no_unusual)[1:4980])
```

```{r}

```


***

